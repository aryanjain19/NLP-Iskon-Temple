{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"Encodings.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"O709zcVkxfa1"},"source":["# Word Encodings"]},{"cell_type":"markdown","metadata":{"id":"C8K_08Pxxfa2"},"source":["Let us say we have collected reviews and determined the sentiment of a movie. \n","\n","Our goal is to create a model to predict the sentiment of the movies based on the text."]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"id":"flEUQpXkxfa3"},"source":["### Setting up data "]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"hidden":true,"id":"qCWnMmPvxfa4"},"source":["#### Creating a data frame"]},{"cell_type":"code","metadata":{"hidden":true,"id":"9LNUa2TAxfa4","colab":{"base_uri":"https://localhost:8080/","height":101},"executionInfo":{"status":"ok","timestamp":1595169016491,"user_tz":-330,"elapsed":1045,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"a584c0fe-7c66-4571-d939-231851368d1e"},"source":["reviews = ['excellent excellent movie','disgusting!','time pass movie','among the all time greats','worst movie. time pass']\n","reviews"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['excellent excellent movie',\n"," 'disgusting!',\n"," 'time pass movie',\n"," 'among the all time greats',\n"," 'worst movie. time pass']"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"hidden":true,"id":"DlMfpHsHxfa7","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595169017834,"user_tz":-330,"elapsed":1319,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"f89c990d-6f91-4422-ee13-5dd2b18f32cf"},"source":["doc_id = ['doc'+str(i+1) for i in (list(range(len(reviews))))]\n","doc_id"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['doc1', 'doc2', 'doc3', 'doc4', 'doc5']"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"hidden":true,"id":"OVj9i4PMxfa9"},"source":["sentiment=[1,0,0,1,0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"hidden":true,"id":"z49lueOfxfa_"},"source":["ds_dict={'doc_id': doc_id, 'reviews': reviews, 'sentiment': sentiment}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"hidden":true,"id":"VMNc5jhkxfbB"},"source":["import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"hidden":true,"id":"VtBa9AcCxfbC","colab":{"base_uri":"https://localhost:8080/","height":195},"executionInfo":{"status":"ok","timestamp":1595169021692,"user_tz":-330,"elapsed":1325,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"d2dac99f-e9de-435b-8c38-a66559bf41fc"},"source":["ds = pd.DataFrame(ds_dict)\n","ds"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>doc_id</th>\n","      <th>reviews</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>doc1</td>\n","      <td>excellent excellent movie</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>doc2</td>\n","      <td>disgusting!</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>doc3</td>\n","      <td>time pass movie</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>doc4</td>\n","      <td>among the all time greats</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>doc5</td>\n","      <td>worst movie. time pass</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  doc_id                    reviews  sentiment\n","0   doc1  excellent excellent movie          1\n","1   doc2                disgusting!          0\n","2   doc3            time pass movie          0\n","3   doc4  among the all time greats          1\n","4   doc5     worst movie. time pass          0"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"_4vzGy3ExfbE"},"source":["#### Creating Vocabulary"]},{"cell_type":"code","metadata":{"hidden":true,"id":"wgck9FfbxfbF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620294136639,"user_tz":-330,"elapsed":3304,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"a14add24-c7f9-4d6e-c263-c83582d099ab"},"source":["import nltk\n","\n","nltk.download('punkt')  \n","from nltk import word_tokenize"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fH-42kmnXI1M"},"source":["lst_words = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"hidden":true,"id":"N1lJQAthxfbG","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595169027580,"user_tz":-330,"elapsed":989,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"6eb207df-fd1f-46f3-9002-863afcef7619"},"source":["for review in reviews:\n","    for token in word_tokenize(review):\n","        lst_words.append(token)\n","\n","vocabulary = set(sorted(lst_words))\n","print(vocabulary)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'.', 'the', 'greats', 'worst', 'disgusting', 'among', 'excellent', '!', 'time', 'pass', 'movie', 'all'}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"id":"hzPxmfwvxfbI"},"source":["### Question: How to represent the textual data?"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"zgBNuE14xfbJ"},"source":["Doc4 = Among the all time greats. \n","\n","This document or review is **ordered sequence of words** and through tokenization we can break this document into words.\n","\n","Doc4 = ['Among', 'the', 'all', 'time', 'greats']"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"_ejZwuMJxfbJ"},"source":["But we see all our data is text and we know that for all the algorithms to work we need to have numeric data. \n","\n","**So how do we convert the text data to numeric data?**"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"0vKeGMeNxfbK"},"source":["In other words, we need each word to have a numeric encoding:\n","\n","'Among' --> x0 (numeric encoding) \n","\n","'the' --> x1 (numeric encoding) \n","\n","'''''\n","\n","But what is that numeric reprsentation?"]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"id":"IIfU2AY1xfbK"},"source":["## One-hot Encoding"]},{"cell_type":"code","metadata":{"id":"kk1VTlh0bnj_","colab":{"base_uri":"https://localhost:8080/","height":195},"executionInfo":{"status":"ok","timestamp":1595169031412,"user_tz":-330,"elapsed":1021,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"eeb9de0c-422c-4119-d11d-333caf40485c"},"source":["columns=['excellent', '.', 'greats', 'the', 'pass', 'worst', 'time', 'disgusting', 'all', '!', 'movie', 'among']\n","\n","features=[[1,0,0,0,0,0,0,0,0,0,1,0],\n","          [0,0,0,0,0,0,0,1,0,1,0,0], \n","          [0,0,0,0,1,0,1,0,0,0,1,0],\n","          [0,0,1,1,0,0,1,0,1,0,0,1],\n","          [0,1,0,0,1,1,1,0,0,0,1,0]]\n","\n","pd.DataFrame(features, columns=columns, index=doc_id)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>excellent</th>\n","      <th>.</th>\n","      <th>greats</th>\n","      <th>the</th>\n","      <th>pass</th>\n","      <th>worst</th>\n","      <th>time</th>\n","      <th>disgusting</th>\n","      <th>all</th>\n","      <th>!</th>\n","      <th>movie</th>\n","      <th>among</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>doc1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>doc2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>doc3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>doc4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>doc5</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      excellent  .  greats  the  pass  ...  disgusting  all  !  movie  among\n","doc1          1  0       0    0     0  ...           0    0  0      1      0\n","doc2          0  0       0    0     0  ...           1    0  1      0      0\n","doc3          0  0       0    0     1  ...           0    0  0      1      0\n","doc4          0  0       1    1     0  ...           0    1  0      0      1\n","doc5          0  1       0    0     1  ...           0    0  0      1      0\n","\n","[5 rows x 12 columns]"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"A-N2X1b7xfbQ"},"source":["### Issues with this encoding"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"SrAP0pbMxfbR"},"source":["- We lose the order/sequence and hence the context.\n","- Here the vocabulary was of 12 words but imagine a case where we have 1 million words as vocabulary. We will end up with massive vocabulary and features/dimensions.\n","- With bigger vocabulary, we end up with high sparsity i.e. most of the cells are empty or 0. For instance, doc2 would be filled for only 2 columns out of 1 million columns of vocabulary.\n","- Since we are just capturing the presence of the word, we lose the frequency information i.e. even if the word is repeated multiple times, we just capture it only once. Doc1 has 'excellent' twice but the value shown is 1.\n","- This does not capture any meaning or relationship between the words.\n"]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"id":"OPDBpzY3xfbR"},"source":["## Count Vectorizer"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"2Z93mcFfxfbS"},"source":["Encodes as the frequency of the word i.e. how often the word is used in the document."]},{"cell_type":"code","metadata":{"hidden":true,"id":"i1aXAaVxxfbT","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1595168416557,"user_tz":-330,"elapsed":1270,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"714ecdc5-4992-486d-9073-7715307a7f21"},"source":["restaurant_reviews = ['the food was very bad','the place was very bad, the food was bad and the service was very bad as well']\n","restaurant_reviews"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['the food was very bad',\n"," 'the place was very bad, the food was bad and the service was very bad as well']"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"hidden":true,"id":"4YP7aFE3xfbV","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595169153327,"user_tz":-330,"elapsed":1070,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"2feef40d-5243-40c9-8e06-30d1aff3cff9"},"source":["rest_review_id = ['doc'+str(i+1) for i in (list(range(len(restaurant_reviews))))]\n","rest_review_id"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['doc1', 'doc2']"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"hidden":true,"id":"q8jPaPiIxfbX","colab":{"base_uri":"https://localhost:8080/","height":106},"executionInfo":{"status":"ok","timestamp":1595169154834,"user_tz":-330,"elapsed":1481,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"88ddc794-4137-4e4b-f212-46219b080a96"},"source":["restaurant_ds=pd.DataFrame({'reviews': restaurant_reviews}, index=rest_review_id)\n","restaurant_ds"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>reviews</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>doc1</th>\n","      <td>the food was very bad</td>\n","    </tr>\n","    <tr>\n","      <th>doc2</th>\n","      <td>the place was very bad, the food was bad and t...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                reviews\n","doc1                              the food was very bad\n","doc2  the place was very bad, the food was bad and t..."]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"hidden":true,"id":"IeroF19WxfbZ","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595169157533,"user_tz":-330,"elapsed":1044,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"53dcf7ec-90c2-4240-d7f8-f54b186ab4bb"},"source":["lst_words=[]\n","for review in restaurant_reviews:\n","    for token in word_tokenize(review):\n","        lst_words.append(token)\n","\n","rest_vocab = set(sorted(lst_words))\n","print(rest_vocab)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'as', 'the', 'service', 'bad', 'and', 'well', 'food', ',', 'was', 'very', 'place'}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"hidden":true,"id":"WNLjs8PMxfbb","colab":{"base_uri":"https://localhost:8080/","height":106},"executionInfo":{"status":"ok","timestamp":1595169161631,"user_tz":-330,"elapsed":1291,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"0e2c0ebe-b9f8-45e3-abc9-54f277a2679e"},"source":["rest_columns=['the', 'well', 'place', 'bad', 'and', 'as', 'very', 'food', 'service', ',', 'was']\n","\n","rest_features=[[1,0,0,1,0,0,1,1,0,0,1],\n","              [3,1,1,3,1,1,2,1,1,1,3]]\n","\n","pd.DataFrame(rest_features, columns=rest_columns, index=rest_review_id)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>the</th>\n","      <th>well</th>\n","      <th>place</th>\n","      <th>bad</th>\n","      <th>and</th>\n","      <th>as</th>\n","      <th>very</th>\n","      <th>food</th>\n","      <th>service</th>\n","      <th>,</th>\n","      <th>was</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>doc1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>doc2</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      the  well  place  bad  and  as  very  food  service  ,  was\n","doc1    1     0      0    1    0   0     1     1        0  0    1\n","doc2    3     1      1    3    1   1     2     1        1  1    3"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"hidden":true,"id":"yJDiLDq9xfbd"},"source":["### Issues with this encoding"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"zrFczPOrxfbe"},"source":["This encoding address the frequency issue of OHE but still has the following issues:\n","\n","- We lose the order/sequence and hence the context.\n","- Here the vocabulary was of 12 words but imagine a case where we have 1 million words as vocabulary. We will end up with massive vocabulary and features/dimensions.\n","- With bigger vocabulary, we end up with high sparsity i.e. most of the cells are empty or 0. For instance, doc2 would be filled for only 2 columns out of 1 million columns of vocabulary.\n","- This does not capture any meaning or relationship between the words."]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"id":"PbPON7X8xfbe"},"source":["## Term Frequency - Inverse Document Frequency (TF-IDF)"]},{"cell_type":"code","metadata":{"hidden":true,"id":"BftkUbwJxfbf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620294111933,"user_tz":-330,"elapsed":1265,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"c71c5f33-2c58-4ff7-878e-c66eea574337"},"source":["stage_reviews = ['the play was good', 'the end was good', 'the cast was brilliant','the ultimate show']\n","stage_reviews"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['the play was good',\n"," 'the end was good',\n"," 'the cast was brilliant',\n"," 'the ultimate show']"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"hidden":true,"id":"l2dnk_u4xfbh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620294113916,"user_tz":-330,"elapsed":1513,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"2a0f1489-12ef-443c-9568-bfb430f68572"},"source":["stage_review_id = ['doc'+str(i+1) for i in (list(range(len(stage_reviews))))]\n","stage_review_id"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['doc1', 'doc2', 'doc3', 'doc4']"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"hidden":true,"id":"HUqycMsfxfbl","colab":{"base_uri":"https://localhost:8080/","height":166},"executionInfo":{"status":"ok","timestamp":1620294115164,"user_tz":-330,"elapsed":1236,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"db7da6ca-3c02-414b-91f1-91358834a470"},"source":["stage_ds=pd.DataFrame({'reviews': stage_reviews}, index=stage_review_id)\n","stage_ds"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>reviews</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>doc1</th>\n","      <td>the play was good</td>\n","    </tr>\n","    <tr>\n","      <th>doc2</th>\n","      <td>the end was good</td>\n","    </tr>\n","    <tr>\n","      <th>doc3</th>\n","      <td>the cast was brilliant</td>\n","    </tr>\n","    <tr>\n","      <th>doc4</th>\n","      <td>the ultimate show</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                     reviews\n","doc1       the play was good\n","doc2        the end was good\n","doc3  the cast was brilliant\n","doc4       the ultimate show"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"hidden":true,"id":"aU7Tqpzixfbm"},"source":["#### Term-Frequency"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"FmoM2proxfbn"},"source":["Term frequency (tf) vector is is the frequency of each token in the document.\n","\n","Term Frequency ($tf_{t,d}$) =  Number of occurrences of word i in the document d i.e. count(t,d)\n","\n","or\n","\n","tf = $log_{10}(count(t,d) + 1)$\n","\n","So, if the count is 1, then tf is log(1 + 1) = 0.3\n","\n","For instance, tf for doc1 is [0.3, 0.3, 0.3, 0.3] for the words [the, play, was, good] respectively.\n"]},{"cell_type":"code","metadata":{"hidden":true,"id":"acz8ejn8xfbn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620294141197,"user_tz":-330,"elapsed":1585,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"0852c5f5-0f14-4c0b-ffbd-07df89ef70e8"},"source":["lst_words=[]\n","for review in stage_reviews:\n","    for token in word_tokenize(review):\n","        lst_words.append(token)\n","\n","stage_vocab = set(sorted(lst_words))\n","print(stage_vocab)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'brilliant', 'show', 'the', 'end', 'was', 'cast', 'ultimate', 'good', 'play'}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"hidden":true,"id":"1DlWXH6yxfbp","colab":{"base_uri":"https://localhost:8080/","height":166},"executionInfo":{"status":"ok","timestamp":1620294149436,"user_tz":-330,"elapsed":1291,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"71d81c60-0f92-4219-b0c4-423adc844ffa"},"source":["stage_columns= ['end', 'show', 'good', 'the', 'ultimate', 'play', 'brilliant', 'cast', 'was']\n","\n","stage_features_tf=[[0,0,0.3,0.3,0,0.3,0,0,0.3],\n","                   [0.3,0,0.3,0.3,0,0,0,0,0.3],\n","                   [0,0,0,0.3,0,0,0.3,0.3,0.3],\n","                   [0,0.3,0,0.3,0.3,0,0,0,0]]\n","\n","pd.DataFrame(stage_features_tf, columns=stage_columns, index=stage_review_id)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>end</th>\n","      <th>show</th>\n","      <th>good</th>\n","      <th>the</th>\n","      <th>ultimate</th>\n","      <th>play</th>\n","      <th>brilliant</th>\n","      <th>cast</th>\n","      <th>was</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>doc1</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.3</td>\n","      <td>0.3</td>\n","      <td>0.0</td>\n","      <td>0.3</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.3</td>\n","    </tr>\n","    <tr>\n","      <th>doc2</th>\n","      <td>0.3</td>\n","      <td>0.0</td>\n","      <td>0.3</td>\n","      <td>0.3</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.3</td>\n","    </tr>\n","    <tr>\n","      <th>doc3</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.3</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.3</td>\n","      <td>0.3</td>\n","      <td>0.3</td>\n","    </tr>\n","    <tr>\n","      <th>doc4</th>\n","      <td>0.0</td>\n","      <td>0.3</td>\n","      <td>0.0</td>\n","      <td>0.3</td>\n","      <td>0.3</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      end  show  good  the  ultimate  play  brilliant  cast  was\n","doc1  0.0   0.0   0.3  0.3       0.0   0.3        0.0   0.0  0.3\n","doc2  0.3   0.0   0.3  0.3       0.0   0.0        0.0   0.0  0.3\n","doc3  0.0   0.0   0.0  0.3       0.0   0.0        0.3   0.3  0.3\n","doc4  0.0   0.3   0.0  0.3       0.3   0.0        0.0   0.0  0.0"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"hidden":true,"id":"mKWiexGyxfbr"},"source":["#### Inverse-Document Frequency"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"IBxj8vzZxfbr"},"source":["It typically measures how important a term is in the corpus. Since tf considers all terms equally important we can’t only use term frequencies to calculate the weight of a term in the document. Besides we know that terms such as “the”, “a”, and “was”, may appear a lot of times and might actually overshadow the important words. Thus we need to reduce the weight of these frequent terms while increase the weight of rare words.\n","\n"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"BxOPHj95xfbs"},"source":["Term frequency is the occurrence count of a term in one particular document only; while document frequency is the number of different documents the term appears in, so it depends on the whole corpus. So, idf of a term is the number of documents in the corpus divided by the document frequency of a term.                                                                                           \n","\n","\n","<center>$idf(t) = \\frac{N}{N(t)}$</center>\n","\n","where,\n","\n","N(t) = Number of documents containing the term t\n","\n","N is the number of documents.\n","\n","It’s expected that the more frequent term to be considered less important. We take the log of the inverse document frequencies to reduce the magnitude, as the actual value could be high.\n","\n","<center>$idf(t) =log_{10}(\\frac{N}{N(t)})$</center>\n","\n"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"9OwO5sY9xfbs"},"source":["For instance, idf('the') = log(4/4) = log(1) = 0 and idf('brilliant') = log(4/1) = 0.6\n","\n","We can see here that the word which is used in every document is reduced to zero and for the rare words in the corpus such as the word 'brilliant' is of higher weight 0.6."]},{"cell_type":"code","metadata":{"hidden":true,"id":"XfTnJ6inxfbt","colab":{"base_uri":"https://localhost:8080/","height":77},"executionInfo":{"status":"ok","timestamp":1620294168225,"user_tz":-330,"elapsed":1841,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"7eebbc9f-76e5-4274-c459-cd8763d9edc4"},"source":["idf_words = [[0.6, 0.6, 0.3, 0, 0.6, 0.6, 0.6, 0.6, 0.125]]\n","\n","pd.DataFrame(idf_words, columns=stage_columns)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>end</th>\n","      <th>show</th>\n","      <th>good</th>\n","      <th>the</th>\n","      <th>ultimate</th>\n","      <th>play</th>\n","      <th>brilliant</th>\n","      <th>cast</th>\n","      <th>was</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.6</td>\n","      <td>0.6</td>\n","      <td>0.3</td>\n","      <td>0</td>\n","      <td>0.6</td>\n","      <td>0.6</td>\n","      <td>0.6</td>\n","      <td>0.6</td>\n","      <td>0.125</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   end  show  good  the  ultimate  play  brilliant  cast    was\n","0  0.6   0.6   0.3    0       0.6   0.6        0.6   0.6  0.125"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"o-Vsalmhxfbu"},"source":["#### TF-IDF"]},{"cell_type":"code","metadata":{"hidden":true,"id":"MRJubFzCxfbu","colab":{"base_uri":"https://localhost:8080/","height":166},"executionInfo":{"status":"ok","timestamp":1620294797060,"user_tz":-330,"elapsed":957,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"a43d9680-27fc-4a63-af7a-e667225b2dbe"},"source":["stage_features_tf_idf=[[0,0,0.09,0,0,0.18,0,0,0.0375], \n","                       [0.18,0,0.09,0,0,0,0,0,0.0375], \n","                       [0,0,0,0,0,0,0.18,0.18,0.0375], \n","                       [0,0.18,0,0,0.18,0,0,0,0]]\n","\n","pd.DataFrame(stage_features_tf_idf, columns=stage_columns, index=stage_review_id)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>end</th>\n","      <th>show</th>\n","      <th>good</th>\n","      <th>the</th>\n","      <th>ultimate</th>\n","      <th>play</th>\n","      <th>brilliant</th>\n","      <th>cast</th>\n","      <th>was</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>doc1</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.09</td>\n","      <td>0</td>\n","      <td>0.00</td>\n","      <td>0.18</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0375</td>\n","    </tr>\n","    <tr>\n","      <th>doc2</th>\n","      <td>0.18</td>\n","      <td>0.00</td>\n","      <td>0.09</td>\n","      <td>0</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0375</td>\n","    </tr>\n","    <tr>\n","      <th>doc3</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.18</td>\n","      <td>0.18</td>\n","      <td>0.0375</td>\n","    </tr>\n","    <tr>\n","      <th>doc4</th>\n","      <td>0.00</td>\n","      <td>0.18</td>\n","      <td>0.00</td>\n","      <td>0</td>\n","      <td>0.18</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       end  show  good  the  ultimate  play  brilliant  cast     was\n","doc1  0.00  0.00  0.09    0      0.00  0.18       0.00  0.00  0.0375\n","doc2  0.18  0.00  0.09    0      0.00  0.00       0.00  0.00  0.0375\n","doc3  0.00  0.00  0.00    0      0.00  0.00       0.18  0.18  0.0375\n","doc4  0.00  0.18  0.00    0      0.18  0.00       0.00  0.00  0.0000"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"HssuY5SDGOnD"},"source":["We observe that the weight are normalized or reduced and that of the word 'the' is 0, even if it is present in all documents."]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"hidden":true,"id":"oFqvbN5Nxfbz"},"source":["### Issues with TF-IDF encoding"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"kDR5MReexfb0"},"source":["This encoding address the frequency issue of OHE and also captures the relative importance of token when compared to Count Vectorizer but still has the following issues:\n","\n","- We lose the order/sequence and hence the context.\n","- Here the vocabulary was of 12 words but imagine a case where we have 1 million words as vocabulary. We will end up with massive vocabulary and features/dimensions.\n","- With bigger vocabulary, we end up with high sparsity i.e. most of the cells are empty or 0. For instance, doc2 would be filled for only 2 columns out of 1 million columns of vocabulary.\n","- This does not capture any meaning or relationship between the words."]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"id":"e20loE11xfb6"},"source":["## Word Embeddings"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"bBY5401txfb6"},"source":["We observed that different techniques were tried but all of them have some major short-comings. \n","\n","Researchers needed embeddings which could simultaneously solve the following issues:\n","\n","- Low dimension matrix\n","- High density and low sparsity\n","- Capture the Context\n","- Semantic Information of words and relations between them\n","\n"]}]}