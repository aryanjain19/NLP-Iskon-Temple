{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"pre-processing.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"heading_collapsed":true,"id":"mvXSlf3XL7fG"},"source":["# Text Pre-processing"]},{"cell_type":"code","metadata":{"hidden":true,"id":"UGnRQEIQL7fH","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1603769857938,"user_tz":-330,"elapsed":3111,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"d40fe269-fb45-46bf-f58b-ddc6bbaf88bb"},"source":["import nltk\n","print(nltk.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["3.2.5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"hidden":true,"id":"VQNi92NdL7fL","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1603769976962,"user_tz":-330,"elapsed":17090,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"dba3eea9-e6ef-4586-d87b-f653704ad079"},"source":["nltk.download()   # Go to Model tab and download Punkt model.\n","\n","#nltk.download('punkt')   # Direct Method"],"execution_count":null,"outputs":[{"output_type":"stream","text":["NLTK Downloader\n","---------------------------------------------------------------------------\n","    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n","---------------------------------------------------------------------------\n","Downloader> l\n","\n","Packages:\n","  [ ] abc................. Australian Broadcasting Commission 2006\n","  [ ] alpino.............. Alpino Dutch Treebank\n","  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n","  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n","  [ ] basque_grammars..... Grammars for Basque\n","  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n","                           Extraction Systems in Biology)\n","  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n","  [ ] book_grammars....... Grammars from NLTK Book\n","  [ ] brown............... Brown Corpus\n","  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n","  [ ] cess_cat............ CESS-CAT Treebank\n","  [ ] cess_esp............ CESS-ESP Treebank\n","  [ ] chat80.............. Chat-80 Data Files\n","  [ ] city_database....... City Database\n","  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n","  [ ] comparative_sentences Comparative Sentence Dataset\n","  [ ] comtrans............ ComTrans Corpus Sample\n","  [ ] conll2000........... CONLL 2000 Chunking Corpus\n","  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n","Hit Enter to continue: \n","  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n","                           and Basque Subset)\n","  [ ] crubadan............ Crubadan Corpus\n","  [ ] dependency_treebank. Dependency Parsed Treebank\n","  [ ] dolch............... Dolch Word List\n","  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n","                           Corpus\n","  [ ] floresta............ Portuguese Treebank\n","  [ ] framenet_v15........ FrameNet 1.5\n","  [ ] framenet_v17........ FrameNet 1.7\n","  [ ] gazetteers.......... Gazeteer Lists\n","  [ ] genesis............. Genesis Corpus\n","  [ ] gutenberg........... Project Gutenberg Selections\n","  [ ] ieer................ NIST IE-ER DATA SAMPLE\n","  [ ] inaugural........... C-Span Inaugural Address Corpus\n","  [ ] indian.............. Indian Language POS-Tagged Corpus\n","  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n","                           ChaSen format)\n","  [ ] kimmo............... PC-KIMMO Data Files\n","  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n","  [ ] large_grammars...... Large context-free and feature-based grammars\n","                           for parser comparison\n","Hit Enter to continue: \n","  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n","  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n","                           part-of-speech tags\n","  [ ] machado............. Machado de Assis -- Obra Completa\n","  [ ] masc_tagged......... MASC Tagged Corpus\n","  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n","  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n","  [ ] moses_sample........ Moses Sample Models\n","  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n","  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n","  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n","                           2015) subset of the Paraphrase Database.\n","  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n","  [ ] nombank.1.0......... NomBank Corpus 1.0\n","  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n","  [ ] nps_chat............ NPS Chat\n","  [ ] omw................. Open Multilingual Wordnet\n","  [ ] opinion_lexicon..... Opinion Lexicon\n","  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n","  [ ] paradigms........... Paradigm Corpus\n","  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n","                           Evaluation Shared Task\n","Hit Enter to continue: \n","  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n","                           character properties in Perl\n","  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n","  [ ] pl196x.............. Polish language of the XX century sixties\n","  [ ] porter_test......... Porter Stemmer Test Files\n","  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n","  [ ] problem_reports..... Problem Report Corpus\n","  [ ] product_reviews_1... Product Reviews (5 Products)\n","  [ ] product_reviews_2... Product Reviews (9 Products)\n","  [ ] propbank............ Proposition Bank Corpus 1.0\n","  [ ] pros_cons........... Pros and Cons\n","  [ ] ptb................. Penn Treebank\n","  [*] punkt............... Punkt Tokenizer Models\n","  [ ] qc.................. Experimental Data for Question Classification\n","  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n","                           version\n","  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n","                           Portuguesa)\n","  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n","  [ ] sample_grammars..... Sample Grammars\n","  [ ] semcor.............. SemCor 3.0\n","Hit Enter to continue: \n","  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n","  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n","  [ ] sentiwordnet........ SentiWordNet\n","  [ ] shakespeare......... Shakespeare XML Corpus Sample\n","  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n","  [ ] smultron............ SMULTRON Corpus Sample\n","  [ ] snowball_data....... Snowball Data\n","  [ ] spanish_grammars.... Grammars for Spanish\n","  [ ] state_union......... C-Span State of the Union Address Corpus\n","  [ ] stopwords........... Stopwords Corpus\n","  [ ] subjectivity........ Subjectivity Dataset v1.0\n","  [ ] swadesh............. Swadesh Wordlists\n","  [ ] switchboard......... Switchboard Corpus Sample\n","  [ ] tagsets............. Help on Tagsets\n","  [ ] timit............... TIMIT Corpus Sample\n","  [ ] toolbox............. Toolbox Sample Files\n","  [ ] treebank............ Penn Treebank Sample\n","  [ ] twitter_samples..... Twitter Samples\n","  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n","                           (Unicode Version)\n","  [ ] udhr................ Universal Declaration of Human Rights Corpus\n","Hit Enter to continue: \n","  [ ] unicode_samples..... Unicode Samples\n","  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n","  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n","  [ ] vader_lexicon....... VADER Sentiment Lexicon\n","  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n","  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n","  [ ] webtext............. Web Text Corpus\n","  [ ] wmt15_eval.......... Evaluation data from WMT15\n","  [ ] word2vec_sample..... Word2Vec Sample\n","  [ ] wordnet............. WordNet\n","  [ ] wordnet_ic.......... WordNet-InfoContent\n","  [ ] words............... Word Lists\n","  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n","                           English Prose\n","\n","Collections:\n","  [ ] all-corpora......... All the corpora\n","  [P] all-nltk............ All packages available on nltk_data gh-pages\n","                           branch\n","  [P] all................. All packages\n","  [P] book................ Everything used in the NLTK Book\n","  [P] popular............. Popular packages\n","Hit Enter to continue: \n","  [ ] tests............... Packages for running tests\n","  [ ] third-party......... Third-party data packages\n","\n","([*] marks installed packages; [P] marks partially installed collections)\n","\n","---------------------------------------------------------------------------\n","    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n","---------------------------------------------------------------------------\n","Downloader> q\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"id":"g4IsTkaKL7fO"},"source":["# Tokenization"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"y7QVNpHRL7fP"},"source":["Tokens are the words (esp. in English) and tokenization is the process of converting the textual data into tokens or words. \n","This is generally the first step in the NLP pipeline. After this step, we perform other tasks such as parsing, taggings, embeddings etc. \n","\n","If we have multiple sentences, we would do sentence tokenization to get the individual sentences and then do word tokenization to get the words per sentence.\n","\n","Let us see with an example:"]},{"cell_type":"code","metadata":{"hidden":true,"id":"BE919FffL7fP"},"source":["quote=\"The richest man is not he who has the most, but he who needs the least. A nice quote.\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"hidden":true,"id":"71h5xJc0L7fR"},"source":["### NLTK - Sentence Tokenization"]},{"cell_type":"markdown","metadata":{"id":"Ot--qFSCY0PG"},"source":["The most useful cues for segmenting a text into sentences are punctuation, like \n","* periods, \n","* question marks, and \n","* exclamation points. \n","\n","Question marks and exclamation points are relatively unambiguous markers of sentence boundaries. Periods, on the other hand, are more ambiguous (it could be sentence boundary marker, or part of words like Mr., Inc., M.Tech etc) \n","\n","So, to address this problem, in general, sentence tokenization methods work by first deciding (based on rules or machine learning; abbreviation dictionary), whether a period is part of the word or is a sentence-boundary marker.\n","\n","Let us see the sentence segmentation using nltk package."]},{"cell_type":"code","metadata":{"hidden":true,"id":"8sizkN3FL7fS"},"source":["from nltk.tokenize import sent_tokenize"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"hidden":true,"id":"5pRimtU9L7fU","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1603769981061,"user_tz":-330,"elapsed":1106,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"ccd96767-d623-4a16-93de-c5e9c5f05ce1"},"source":["sentences=sent_tokenize(quote)\n","print(sentences)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['The richest man is not he who has the most, but he who needs the least.', 'A nice quote.']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"hidden":true,"id":"atgNmMSjL7fW"},"source":["### NTLK - Word Tokenization"]},{"cell_type":"code","metadata":{"id":"mvMueyrqGQts"},"source":["from nltk.tokenize import word_tokenize"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"hidden":true,"id":"_E2gpZCiL7fW","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1603769985402,"user_tz":-330,"elapsed":1039,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"130cd141-841c-4f19-ff04-31a66b79c83c"},"source":["words=[word_tokenize(sentence) for sentence in sentences]\n","print(words)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[['The', 'richest', 'man', 'is', 'not', 'he', 'who', 'has', 'the', 'most', ',', 'but', 'he', 'who', 'needs', 'the', 'least', '.'], ['A', 'nice', 'quote', '.']]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"hidden":true,"id":"EQ94u4vJL7fZ"},"source":["### Spacy -- Tokenization"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"WbrpOtKZL7fZ"},"source":["The nlp object:\n","\n","- contains the processing pipeline\n","- includes language-specific rules for tokenization"]},{"cell_type":"code","metadata":{"hidden":true,"id":"qEwZvH4EL7fa"},"source":["from spacy.lang.en import English\n","nlp = English()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"lgingWigL7fc"},"source":["**Doc Object**"]},{"cell_type":"code","metadata":{"hidden":true,"id":"Doc-02jnL7fc","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1603769993638,"user_tz":-330,"elapsed":1068,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"2d753c5b-a415-424b-8503-9a1ea0969fc3"},"source":["# Created by processing a string of text with the nlp object\n","doc = nlp(\"Hello world!\")\n","\n","# Iterate over tokens in a Doc\n","for token in doc:\n","    print(token.text)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Hello\n","world\n","!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"FaGU5EdDL7ff"},"source":["**Token Object**"]},{"cell_type":"code","metadata":{"hidden":true,"id":"R25m6ODfL7fg","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1603769996179,"user_tz":-330,"elapsed":1003,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"b6d8c788-d49d-4b86-dadd-c3724af57716"},"source":["# Index into the Doc to get a single Token\n","token = doc[1]\n","# Get the token text via the .text attribute\n","print(token.text)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["world\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"hidden":true,"id":"hFkQBU_ML7fi"},"source":["### Exercise"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"hwkH5_xZL7fi"},"source":["Create list of list of tokens using NLTK, for the following sentences."]},{"cell_type":"code","metadata":{"hidden":true,"id":"edOsW2nML7fj"},"source":["brilliant_quote=\"It always seems impossible until it’s done. A quote by Nelson Mandela.\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"hidden":true,"id":"vHW2gkwJL7fl","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1603770000046,"user_tz":-330,"elapsed":941,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"754c091c-747d-4903-99e2-a1b299b63b1d"},"source":["print([word_tokenize(sentence) for sentence in sent_tokenize(brilliant_quote)])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[['It', 'always', 'seems', 'impossible', 'until', 'it', '’', 's', 'done', '.'], ['A', 'quote', 'by', 'Nelson', 'Mandela', '.']]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"mhlZO65OL7fn"},"source":["#### Create a single list of tokens"]},{"cell_type":"code","metadata":{"hidden":true,"id":"N_nPNsTIL7fn","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1603770002251,"user_tz":-330,"elapsed":1257,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"5624fa82-7d3f-4615-a7d5-ce6a5cbb14c5"},"source":["print([word.lower() for sent in sent_tokenize(brilliant_quote) for word in word_tokenize(sent)])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['it', 'always', 'seems', 'impossible', 'until', 'it', '’', 's', 'done', '.', 'a', 'quote', 'by', 'nelson', 'mandela', '.']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"1KGJze-IL7fp"},"source":["#### Create a unique single set of tokens"]},{"cell_type":"code","metadata":{"hidden":true,"id":"H6jXuQsUL7fp","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1603770003631,"user_tz":-330,"elapsed":1025,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"606427e4-c775-4290-a08d-3a21d472ec72"},"source":["print(set([word.lower() for sent in sent_tokenize(brilliant_quote) for word in word_tokenize(sent)]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'.', 'impossible', 'seems', 'it', 'until', 'mandela', 's', '’', 'done', 'always', 'by', 'a', 'quote', 'nelson'}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"id":"ayJD10C_L7fr"},"source":["# Stopwords"]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"hidden":true,"id":"9vyE5pleL7fs"},"source":["### NLTK"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"saGbZhdmL7fs"},"source":["From [Intro to Information Retrieval](https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html):\n","\n","*Some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called stop words.*\n","\n","*The general trend in IR systems over time has been from standard use of quite large stop lists (200-300 terms) to very small stop lists (7-12 terms) to no stop list whatsoever. Web search engines generally do not use stop lists.*"]},{"cell_type":"code","metadata":{"hidden":true,"id":"dsGTKLSoL7fs"},"source":["from nltk.corpus import stopwords\n","from string import punctuation"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"hidden":true,"id":"WXkyeaIgL7fu","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1603770007150,"user_tz":-330,"elapsed":707,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"6ffc8dd0-74b7-41e9-a592-e235f27f36bb"},"source":["print(list(punctuation))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8ulslXFlPNyy","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1603770008899,"user_tz":-330,"elapsed":1180,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"ada7b203-e961-44a9-e4ae-eb80c8f170ea"},"source":["nltk.download('stopwords')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"hidden":true,"id":"HrOuKtBUL7fw","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1603770009282,"user_tz":-330,"elapsed":758,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"74fc80b9-4ce2-407a-8725-9b079364cefe"},"source":["set_of_stop_words = set(stopwords.words('english') + list(punctuation))\n","set_of_stop_words"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'!',\n"," '\"',\n"," '#',\n"," '$',\n"," '%',\n"," '&',\n"," \"'\",\n"," '(',\n"," ')',\n"," '*',\n"," '+',\n"," ',',\n"," '-',\n"," '.',\n"," '/',\n"," ':',\n"," ';',\n"," '<',\n"," '=',\n"," '>',\n"," '?',\n"," '@',\n"," '[',\n"," '\\\\',\n"," ']',\n"," '^',\n"," '_',\n"," '`',\n"," 'a',\n"," 'about',\n"," 'above',\n"," 'after',\n"," 'again',\n"," 'against',\n"," 'ain',\n"," 'all',\n"," 'am',\n"," 'an',\n"," 'and',\n"," 'any',\n"," 'are',\n"," 'aren',\n"," \"aren't\",\n"," 'as',\n"," 'at',\n"," 'be',\n"," 'because',\n"," 'been',\n"," 'before',\n"," 'being',\n"," 'below',\n"," 'between',\n"," 'both',\n"," 'but',\n"," 'by',\n"," 'can',\n"," 'couldn',\n"," \"couldn't\",\n"," 'd',\n"," 'did',\n"," 'didn',\n"," \"didn't\",\n"," 'do',\n"," 'does',\n"," 'doesn',\n"," \"doesn't\",\n"," 'doing',\n"," 'don',\n"," \"don't\",\n"," 'down',\n"," 'during',\n"," 'each',\n"," 'few',\n"," 'for',\n"," 'from',\n"," 'further',\n"," 'had',\n"," 'hadn',\n"," \"hadn't\",\n"," 'has',\n"," 'hasn',\n"," \"hasn't\",\n"," 'have',\n"," 'haven',\n"," \"haven't\",\n"," 'having',\n"," 'he',\n"," 'her',\n"," 'here',\n"," 'hers',\n"," 'herself',\n"," 'him',\n"," 'himself',\n"," 'his',\n"," 'how',\n"," 'i',\n"," 'if',\n"," 'in',\n"," 'into',\n"," 'is',\n"," 'isn',\n"," \"isn't\",\n"," 'it',\n"," \"it's\",\n"," 'its',\n"," 'itself',\n"," 'just',\n"," 'll',\n"," 'm',\n"," 'ma',\n"," 'me',\n"," 'mightn',\n"," \"mightn't\",\n"," 'more',\n"," 'most',\n"," 'mustn',\n"," \"mustn't\",\n"," 'my',\n"," 'myself',\n"," 'needn',\n"," \"needn't\",\n"," 'no',\n"," 'nor',\n"," 'not',\n"," 'now',\n"," 'o',\n"," 'of',\n"," 'off',\n"," 'on',\n"," 'once',\n"," 'only',\n"," 'or',\n"," 'other',\n"," 'our',\n"," 'ours',\n"," 'ourselves',\n"," 'out',\n"," 'over',\n"," 'own',\n"," 're',\n"," 's',\n"," 'same',\n"," 'shan',\n"," \"shan't\",\n"," 'she',\n"," \"she's\",\n"," 'should',\n"," \"should've\",\n"," 'shouldn',\n"," \"shouldn't\",\n"," 'so',\n"," 'some',\n"," 'such',\n"," 't',\n"," 'than',\n"," 'that',\n"," \"that'll\",\n"," 'the',\n"," 'their',\n"," 'theirs',\n"," 'them',\n"," 'themselves',\n"," 'then',\n"," 'there',\n"," 'these',\n"," 'they',\n"," 'this',\n"," 'those',\n"," 'through',\n"," 'to',\n"," 'too',\n"," 'under',\n"," 'until',\n"," 'up',\n"," 've',\n"," 'very',\n"," 'was',\n"," 'wasn',\n"," \"wasn't\",\n"," 'we',\n"," 'were',\n"," 'weren',\n"," \"weren't\",\n"," 'what',\n"," 'when',\n"," 'where',\n"," 'which',\n"," 'while',\n"," 'who',\n"," 'whom',\n"," 'why',\n"," 'will',\n"," 'with',\n"," 'won',\n"," \"won't\",\n"," 'wouldn',\n"," \"wouldn't\",\n"," 'y',\n"," 'you',\n"," \"you'd\",\n"," \"you'll\",\n"," \"you're\",\n"," \"you've\",\n"," 'your',\n"," 'yours',\n"," 'yourself',\n"," 'yourselves',\n"," '{',\n"," '|',\n"," '}',\n"," '~'}"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"hidden":true,"id":"MWLM7hLeL7fy","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1603770011902,"user_tz":-330,"elapsed":983,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"a873e273-488f-4788-ac6b-7cb261937c96"},"source":["non_stop_words = [word for word in word_tokenize(quote) if word not in set_of_stop_words]\n","print(non_stop_words)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['The', 'richest', 'man', 'needs', 'least', 'A', 'nice', 'quote']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"hidden":true,"id":"7QF8k7FLL7f0"},"source":["### Spacy"]},{"cell_type":"code","metadata":{"hidden":true,"id":"rK2g36_AL7f0"},"source":["import spacy\n","nlp = spacy.load(\"en_core_web_sm\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"hidden":true,"id":"bMV5i1FoL7f2","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1603770016357,"user_tz":-330,"elapsed":1244,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"a7944ca1-f53c-46a1-8a9d-aaa182719e22"},"source":["print(sorted(list(nlp.Defaults.stop_words))[:20])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"hidden":true,"id":"lO46YKLLL7f4"},"source":["### Exercise"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"Y-tf7nAcL7f4"},"source":["Print all those stop words which are there in Spacy's stopwords but not there in NLTK's version"]},{"cell_type":"code","metadata":{"hidden":true,"id":"D9X5p2EgL7f4","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1595164935325,"user_tz":-330,"elapsed":891,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"a77f883d-625c-44fe-cc9d-5764db9b9250"},"source":["print(nlp.Defaults.stop_words - set_of_stop_words)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'becoming', 'elsewhere', 'might', 'nevertheless', 'indeed', 'still', 'n‘t', 'six', 'noone', 'last', 'latterly', 'except', 'nobody', 'whether', 'top', 'thus', 'others', 'sometime', 'cannot', 'therefore', 'whatever', 'rather', 'serious', 'whither', \"'ll\", 'otherwise', 'always', 'upon', 'per', 'five', 'seems', 'either', 'using', 'thru', 'however', \"n't\", 'thereby', 'unless', 'nine', 'hence', 'would', 'front', 'several', '‘ve', 'throughout', 'sixty', 'could', 'beside', 'within', 'move', 'around', \"'re\", 'also', 'whoever', 'since', 'thereupon', 'back', 'ten', 'amount', '’s', 'amongst', 'enough', 'forty', 'every', 'namely', 'see', 'keep', 'made', 'seem', \"'s\", 'quite', 'former', 'somewhere', 'put', 'none', 'various', \"'ve\", 'hundred', 'towards', 'used', 'never', 'hereafter', 'make', 'whose', 'perhaps', 'herein', 'already', \"'d\", 'almost', 'thereafter', 'anything', 'everyone', 'must', '’m', 'neither', 'without', 'sometimes', 'among', 'via', 'hereupon', 'mostly', 'side', 'eleven', 'mine', 'least', 'call', '’re', 'fifty', 'even', 'third', 'name', 'something', 'therein', 'seeming', '‘re', 'anywhere', 'take', 'whole', 'afterwards', '’d', 'besides', '’ve', 'meanwhile', 'give', 'moreover', 'next', 'us', 'whereby', 'wherever', 'another', 'nowhere', 'part', 'well', 'whenever', '’ll', 'else', 'formerly', \"'m\", 'say', 'become', 'though', 'may', 'less', 'much', 'became', 'show', 'twelve', 'one', 'someone', 'onto', 'regarding', 'ever', 'ca', 'three', 'yet', 'please', 'anyway', 'latter', 'get', 'often', 'really', 'thence', 'twenty', 'although', 'along', 'beyond', 'nothing', 'beforehand', 'hereby', 'anyone', 'full', 'empty', 'across', 'becomes', 'whereas', '‘ll', 'whereafter', '‘s', 'n’t', 'seemed', 'done', 'toward', 'go', 'whence', 'first', 'two', 'behind', 'many', 'everything', 'alone', 'somehow', 'four', 'together', 'anyhow', 'bottom', 'everywhere', 'wherein', 'eight', '‘d', 'fifteen', 'due', '‘m', 'whereupon'}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"hidden":true,"id":"L0XfNahVL7f8"},"source":["### Observation"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"fYY8qhNoL7f8"},"source":["Each one has its own stop words. There is no single universal list of stop words."]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"hidden":true,"id":"22xVLGYML7f8"},"source":["### Guidelines"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"KrXwUTXgL7f9"},"source":["**When do we use stopwords?**\n","\n","Recent trends show less usage of stopwords. It is generally used, when stop words do not add lot of value. For instance, when we are doing topic modeling, we are interested in the words which are not stop words, to represent the topic (top used words). However, for the sentiment analysis, each word might have a meaning and would be better not to avoid it. Adding an exclamation sign, could actually change the sentiment.\n"]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"id":"rwAzjfr9L7f9"},"source":["# Stemming and Lemmatization"]},{"cell_type":"code","metadata":{"id":"8qxjqyVL00VH"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"nb5Lp6hwL7f9"},"source":["from [Information Retrieval](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) textbook:\n","\n","For grammatical reasons, documents are going to use different forms of a word, such as *organize, organizes, and organizing*.\n","Additionally, there are families of derivationally related words with similar meanings, such as democracy, democratic, and democratization.\n","\n","**The goal of both stemming and lemmatization** is to reduce grammatical forms and sometimes derivationally related forms of a word to a **common base form**. \n","\n","For instance:\n","\n","- am, are, is $\\Rightarrow$ be \n","\n","- car, cars, car's, cars' $\\Rightarrow$ car\n","\n"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"LrLFAa6xL7f-"},"source":["**Stemming** usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. \n","\n","**Lemmatization** usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove grammatical endings only and to return the base or dictionary form of a word, which is known as the **lemma** . \n","\n","If confronted with the token **saw, stemming might return just s**, whereas **lemmatization would attempt to return either see or saw** depending on whether the use of the token was as a verb or a noun."]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"hidden":true,"id":"BJ_pyVNbL7f-"},"source":["### NLTK -  Stemming"]},{"cell_type":"code","metadata":{"hidden":true,"id":"F8fU1iKrL7f_"},"source":["another_sentence = \"organize, organizes, and organizing are variations of the word organize.\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"hidden":true,"id":"uOkY_cR8L7gA"},"source":["#### Lancaster Stemmer"]},{"cell_type":"code","metadata":{"hidden":true,"id":"oPX-KJ5WL7gB"},"source":["from nltk.stem.lancaster import LancasterStemmer\n","ls=LancasterStemmer()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"hidden":true,"id":"iHnm8ge-L7gC","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1603770033162,"user_tz":-330,"elapsed":1188,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"774e57c4-85de-4c33-fe09-bd1ced4d23a8"},"source":["stem_words=[ls.stem(word) for word in word_tokenize(another_sentence)]\n","print(stem_words)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['org', ',', 'org', ',', 'and', 'org', 'ar', 'vary', 'of', 'the', 'word', 'org', '.']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"pjaaxZmLL7gE"},"source":["Looks like this model truncates quite a lot. The stemmed words lose lot of information."]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"hidden":true,"id":"-4qX2ZMzL7gE"},"source":["#### Porter Stemmer"]},{"cell_type":"code","metadata":{"hidden":true,"id":"48DgrNlOL7gF"},"source":["from nltk.stem.porter import PorterStemmer\n","ps = PorterStemmer()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"hidden":true,"id":"-apoLM6wL7gH","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1603770035376,"user_tz":-330,"elapsed":1022,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"32ee7530-5b0b-4560-c285-bfa33375197d"},"source":["stem_porter_words=[ps.stem(word) for word in word_tokenize(another_sentence)]\n","print(stem_porter_words)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['organ', ',', 'organ', ',', 'and', 'organ', 'are', 'variat', 'of', 'the', 'word', 'organ', '.']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"pRdO872VL7gK"},"source":["Looks like the model just truncates 'ize', 'ise', 'ion' part of the word.\n","\n","Not that sophisticated."]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"hidden":true,"id":"OD1PZWFwL7gK"},"source":["#### Snowball Stemmer"]},{"cell_type":"code","metadata":{"hidden":true,"id":"Sv38_1hDL7gK"},"source":["from nltk.stem.snowball import SnowballStemmer    \n","ss = SnowballStemmer('english')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"hidden":true,"id":"OuqQHTaQL7gM","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1603770038714,"user_tz":-330,"elapsed":1195,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"f2076754-bf02-4014-b25e-06189a153c81"},"source":["stem_snowball_words=[ss.stem(word) for word in word_tokenize(another_sentence)]\n","print(stem_snowball_words)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['organ', ',', 'organ', ',', 'and', 'organ', 'are', 'variat', 'of', 'the', 'word', 'organ', '.']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"frEW1HJ8L7gN"},"source":["Similar to Porter Stemmer.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"hidden":true,"id":"Jcbx-xuaL7gO"},"source":["### NLTK - Lemmetization"]},{"cell_type":"code","metadata":{"hidden":true,"id":"9ozryKmfL7gO","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1603770042327,"user_tz":-330,"elapsed":1817,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"a50980fb-b84b-4ef9-a98b-893b5aaafb3a"},"source":["nltk.download('wordnet')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"hidden":true,"id":"OjWdOd6kL7gQ"},"source":["from nltk.stem import WordNetLemmatizer\n","wl = WordNetLemmatizer()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"hidden":true,"id":"I5DO5mfDL7gR","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1603770046626,"user_tz":-330,"elapsed":2700,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"b1aef836-8451-4f8d-d191-32eee936336d"},"source":["lemma_wordnet_words=[wl.lemmatize(word) for word in word_tokenize(another_sentence)]\n","print(lemma_wordnet_words)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['organize', ',', 'organizes', ',', 'and', 'organizing', 'are', 'variation', 'of', 'the', 'word', 'organize', '.']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"hidden":true,"id":"xazJe2myL7gT"},"source":["### Spacy - Lemmetization"]},{"cell_type":"code","metadata":{"hidden":true,"id":"HS5QpQzOL7gU"},"source":["from spacy.lemmatizer import Lemmatizer\n","lemmatizer = nlp.Defaults.create_lemmatizer()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"hidden":true,"id":"rO21fB7LL7gV","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1603770051709,"user_tz":-330,"elapsed":997,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"9a123c55-b893-4f99-8afd-30ef07dd8ee3"},"source":["print([lemmatizer.lookup(token) for token in nlp(another_sentence)])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[organize, ,, organizes, ,, and, organizing, are, variations, of, the, word, organize, .]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"rAsSXGs9L7gX"},"source":["Spacy doesn't offer a stemmer as lemmatization is considered better."]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"hidden":true,"id":"pj4e7k73L7gX"},"source":["### Exercise"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"lt8ndMsoL7gX"},"source":["Use NLTK - porter stemmer and wordnet lemmatizer"]},{"cell_type":"code","metadata":{"hidden":true,"id":"pw6Gs7DrL7gY"},"source":["sentence_cry ='cry, cries, cried and crying are variations of the word cry'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"yjCNkCdpL7gb"},"source":["Print the stemmed words"]},{"cell_type":"code","metadata":{"hidden":true,"id":"eKq7OZ24L7gb","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1603770056544,"user_tz":-330,"elapsed":993,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"c1ce7e8a-d665-4816-e316-a2a7e779d919"},"source":["print([ps.stem(word) for word in word_tokenize(sentence_cry)])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['cri', ',', 'cri', ',', 'cri', 'and', 'cri', 'are', 'variat', 'of', 'the', 'word', 'cri']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"wIzUmoUyL7gd"},"source":["Print the lemmatized word"]},{"cell_type":"code","metadata":{"hidden":true,"id":"qw_maTZUL7gd","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1603770058153,"user_tz":-330,"elapsed":1027,"user":{"displayName":"naveen b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfDuIilaUa2Wk0R0x2CQ5B-rp_xEt0Ll-QBw6X=s64","userId":"07342808041236324682"}},"outputId":"920a4f8b-19f5-4338-8b10-c3db7adeb611"},"source":["print([wl.lemmatize(word) for word in word_tokenize(sentence_cry)])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['cry', ',', 'cry', ',', 'cried', 'and', 'cry', 'are', 'variation', 'of', 'the', 'word', 'cry']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"hidden":true,"id":"XsX4GjRfL7gf"},"source":["### Observation"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"3KjeyvmcL7gf"},"source":["Stemming and Lemmatization both generate the root form of the words. \n","\n","Lemmatization uses the rules about a language.  The resulting tokens are all actual words.\n"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"6gCidoRCL7gg"},"source":["### Guidelines"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"BI13c_hIL7gg"},"source":["As mentioned above Stemming is a crude heuristic that chops the ends off of words but the resulting tokens may not be actual words. However, Stemming is faster and is generally used when computation power is less.\n","\n","For better results, lemmetization is better, but it is slower and computationally costlier.\n","\n","**When do we do stemming/lemmatization?**\n","\n","Again, it depends on the context. For topic modeling exercise, might be useful and could be done. However, for sentiment analysis, it is better avoided.\n"]}]}